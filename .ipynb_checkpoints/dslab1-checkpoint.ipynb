{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Python Numerical stack and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "\n",
    "- Learning Goals\n",
    "- Introduction to Numpy\n",
    "- Introduction to Pandas\n",
    "- Beginning Exploratory Data Analysis (EDA)\n",
    "\n",
    "\n",
    "## Part 0: Learning Goals \n",
    "We load a dataset first as a numpy array and then as a pandas dataframe, and begin exploratory data analysis (EDA). \n",
    "By the end of this lab, you will be able to:\n",
    "- Create and manipulate one-dimensional and two-dimensional numpy arrays, and pandas series and dataframes.\n",
    "- Describe how to index and \"type\" Pandas Series and Dataframes.\n",
    "- Create histograms and scatter plots for basic exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To draw things on the notebook instead of a separate window\n",
    "%matplotlib inline \n",
    "# Import necessary libraries\n",
    "import numpy as np # imports a fast numerical programming library\n",
    "import scipy as sp #imports stats functions, amongst other things\n",
    "import matplotlib as mpl # this actually imports matplotlib\n",
    "import matplotlib.cm as cm # allows us easy access to colormaps\n",
    "import matplotlib.pyplot as plt # sets up plotting under plt\n",
    "import pandas as pd #lets us handle data as dataframes\n",
    "# sets up pandas table display\n",
    "#pd.set_option('display.width', 500)\n",
    "#pd.set_option('display.max_columns', 100)\n",
    "#pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns #sets up styles and gives us more plotting options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction to Numpy\n",
    "Scientific Python code uses a fast array structure - the numpy array(0-indexed) which is listy! We can compute length, slice, and iterate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_array = np.array([1, 2, 3, 4]) #create\n",
    "my_array             # display\n",
    "print(len(my_array)) # length\n",
    "print(my_array[2:4]) # slice\n",
    "for ele in my_array: # loop\n",
    "    print(ele)\n",
    "print(my_array.mean()) # calcuate mean by method call\n",
    "print(np.mean(my_array)) # calculate mean using numpy \n",
    "np.ones(10) # generates 10 floating point ones\n",
    "np.ones(10, dtype='int') # generates 10 integer ones\n",
    "np.zeros(10)\n",
    "np.random.random(10) # uniform on [0,1]\n",
    "# generate random numbers from a normal distribution with mean 0 and variance 1\n",
    "normal_array = np.random.randn(1000)\n",
    "print(\"The sample mean = %f standard devation = %f\" %(np.mean(normal_array), np.std(normal_array)))\n",
    "#numpy supports vector operations \n",
    "first = np.ones(5)\n",
    "second = np.ones(5)\n",
    "first + second\n",
    "first + 1\n",
    "first*5\n",
    "# if you wanted the distribution N(5,7) you could do:\n",
    "normal_5_7 = 5 + 7*normal_array\n",
    "np.mean(normal_5_7), np.std(normal_5_7)\n",
    "\n",
    "ones_2d = np.ones([3, 4]) # 3 x 4 array of ones\n",
    "ones_2d.shape # show size\n",
    "ones_2d.T # trasnpose\n",
    "np.sum(normal_5_7) # sum all elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Introduction to Pandas \n",
    "\n",
    "Often data is stored in comma separated values (CSV) files. CSV files can be output by any spreadsheet software, and are plain text, hence are a great way to share data. \n",
    "### Importing data with numpy\n",
    "Below we'll read in automobile data from a CSV file, storing the data in Python's memory first as a numpy array.  \n",
    "**Read car_data_description first.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrcars = np.genfromtxt('data/mtcars.csv', delimiter=',', skip_header=1, usecols=(1,2,3,4,5,6,7,8,9,10,11))\n",
    "print(arrcars.shape)\n",
    "print(arrcars[0:2]) # not very nice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a data structure that can represent the columns in the data by their name, can easily store variables of different types, that stores column names, and that we can reference by column name as well as by indexed position and have built-in functions that we can use to manipulate it. \n",
    "Pandas is a package/library that does all of this!  The library is built on top of numpy.  There are two basic pandas objects, *series* and *dataframes*, which can be thought of as enhanced versions of 1D and 2D numpy arrays, respectively.  Pandas attempts to keep all the efficiencies that `numpy` gives us.\n",
    "### Importing data with pandas\n",
    "Now let's read in our automobile data as a pandas *dataframe* structure. And look into the first five rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the csv files\n",
    "data = pd.read_csv(\"data/mtcars.csv\")\n",
    "type(dfcars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have now is a spreadsheet with indexed rows and named columns, called a *dataframe* in pandas.  `data` is an *instance* of the pd.DataFrame *class*, created by calling the pd.read_csv and it has methods (functions) belonging to it. \n",
    "A pandas dataframe is a set of columns pasted together into a spreadsheet. The columns in pandas are called *series* objects.\n",
    "Notice the poorly named first column: \"Unnamed: 0\". This happened because the first column does not have a name.    \n",
    "\"\",\"mpg\",\"cyl\",\"disp\",\"hp\",\"drat\",\"wt\",\"qsec\",\"vs\",\"am\",\"gear\",\"carb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lets clean that up by renaming it to 'name'\n",
    "data = data.rename(columns={\"Unnamed: 0\": \"name\"})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#To access a *series* (column), you can use either dictionary syntax or instance-variable syntax.\n",
    "data.mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#You can get a numpy array of values from the Pandas Series:\n",
    "data.mpg.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And we can produce a histogram from these values\n",
    "plt.hist(data.mpg.values, bins=20);\n",
    "plt.xlabel(\"mpg\");\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Miles per Gallon\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can get a histogram using panda\n",
    "data.mpg.hist(bins=20);\n",
    "plt.xlabel(\"mpg\");\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Miles per Gallon\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas supports a dictionary like access to columns. \n",
    "data['mpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also get sub-dataframes by choosing a set of series.  \n",
    "data[['am', 'mpg']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes and Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our automobile data loaded as a dataframe, we'd like to be able to manipulate it, its series, and its sub-dataframes, say by calculating statistics and plotting distributions of features.  Like arrays and other containers, dataframes and series are listy, so we can apply the list operations we already know to these new containers.  Below we explore our dataframe and its properties, in the context of listiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Listiness property 1: set length\n",
    "print(data.shape)     # 12 columns, each of length 32\n",
    "print(len(data))      # the number of rows in the dataframe, also the length of a series\n",
    "print(len(data.mpg))  # the length of a series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listiness property 2: iteration via loops\n",
    "# One consequence of the column-wise construction of dataframes is that you cannot easily iterate over the rows. \n",
    "# Instead, we iterate over the columns. \n",
    "for column in data: # iterating iterates over column names though, like a dictionary\n",
    "    print(column)\n",
    "    \n",
    "# Or we can call the attribute `columns`.\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can iterate series in the same way that we iterate lists. \n",
    "# Here we print out the number of cylinders for each of the 32 vehicles. \n",
    "for element in data.cyl:\n",
    "    print(element)\n",
    "# you can iterate over rows by using `itertuples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listiness property 3: slice\n",
    "print(list(data.index)) # index for the dataframe\n",
    "data.cyl.index # index for the cyl series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to index dataframes. The `loc` property indexes by label name, while `iloc` indexes by position in the index.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.iloc[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[2:5, 1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[7:9, ['mpg', 'cyl', 'disp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add another column named 'maker' by parsing the first column\n",
    "data['maker'] = data.name.apply(lambda x: x.split()[0])\n",
    "data['maker']\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's make a toy dataframe from scratch.** \n",
    "- Create a series called `column_1` with entries 0, 1, 2, 3.\n",
    "- Create a second series called `column_2` with entries 4, 5, 6, 7.\n",
    "- Glue these series into a dataframe called `table`, where the first and second labelled column of the dataframe are `column_1` and `column_2`, respectively.  In the dataframe, `column_1` should be indexed as `col_1` and `column_2` should be indexed as `col_2`.\n",
    "- Oops!  You've changed your mind about the index labels for the columns.  Use `rename` to rename `col_1` as `Col_1` and `col_2` as `Col_2`. \n",
    "- Rename `0` as `zero`, `1` as `one`, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_1 = pd.Series(range(4))\n",
    "column_2 = pd.Series(range(4,8))\n",
    "table = pd.DataFrame({'col_1': column_1, 'col_2': column_2})\n",
    "table = table.rename(columns={\"col_1\": \"Col_1\", \"col_2\":\"Col_2\"})\n",
    "table\n",
    "# try this\n",
    "#table = table.rename({0: \"zero\", 1: \"one\", 2: \"two\", 3: \"three\"})\n",
    "#table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types\n",
    "\n",
    "Columns in a dataframe (series) come with their own types. Some data may be categorical, boolean, or integer, floating-point, and `object`. The latter is a catch-all for a string or anything Pandas cannot infer, for example, a column that contains data of mixed types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical \n",
    "data.maker.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.maker.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_mpg = data.groupby('maker').mpg.mean()\n",
    "av_mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query\n",
    "data.mpg < 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.mpg < 100].head() #try other queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.query(\"10 <= mpg <= 50\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by=\"mpg\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.gear == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.mpg.max()\n",
    "# find sum, mean etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(\"maker\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 3:  Exploratory Data Analysis (EDA) - Global Properties\n",
    "\n",
    "Below is a basic checklist for the early stages of exploratory data analysis in Python. While not universally applicable, the rubric covers patterns which recur in several data analysis contexts, so useful to keep it in mind when encountering a new dataset.\n",
    "1. **Build** a DataFrame from the data (ideally, put all data in this object)\n",
    "2. **Clean** the DataFrame. It should have the following properties:\n",
    "    - Each row describes a single object\n",
    "    - Each column describes a property of that object\n",
    "    - Columns are numeric whenever appropriate\n",
    "    - Columns contain atomic properties that cannot be further decomposed    \n",
    "3. Explore **global properties**. Use histograms, scatter plots, and aggregation functions to summarize the data.\n",
    "4. Explore **group properties**. Use groupby and small multiples to compare subsets of the data.\n",
    "\n",
    "This process transforms the data into a format which is easier to work with, gives you a basic overview of the data's properties, and likely generates several questions for you to follow-up on in subsequent analysis.\n",
    "So far we have **built** the dataframe from automobile data, and carried out very minimal **cleaning** (renaming) in this dataframe.  We'll now visualize global  properties of our dataset.  We illustrate the concepts using `mpg`. \n",
    "### Histograms\n",
    "A histogram shows the frequency distribution of a dataset.  Below is the distribution of `mpg`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.mpg.plot.hist()  \n",
    "plt.xlabel(\"mpg\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data.mpg, bins=20);\n",
    "plt.xlabel(\"mpg\");\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Miles per Gallon\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**: Plot the distribution of the rear axle ratio (`drat`).  Label the axes accordingly and give the plot a title.    Calculate the mean of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drat.plot.hist();\n",
    "plt.xlabel(\"drat\");\n",
    "plt.ylabel(\"Frequency\");\n",
    "plt.title(\"Rear axle ratio (drat)\");\n",
    "print(\"mean = \", data.drat.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots\n",
    "We often want to see co-variation among our columns, for example, miles/gallon versus weight.  This can be done with a scatter plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data.wt, data.mpg); # you could also use plot and plot data as dots, try that.\n",
    "plt.xlabel(\"weight\");\n",
    "plt.ylabel(\"miles per gallon\"); # plt.show() if you run your Python program from a file. \n",
    "#plt.savefig('images/foo1.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a new dataframe with the columns of interest, sort it based on the x-value (`wt` in this case), and plot the sorted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = data[['wt', 'mpg']]\n",
    "data_temp = sub_data.sort_values('wt')\n",
    "plt.plot(data_temp.wt, data_temp.mpg, 'o-');\n",
    "plt.xlabel(\"weight\");\n",
    "plt.ylabel(\"miles per gallon\"); #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**: Create a scatter plot showing the co-variation between two columns of your choice. Label the axes. Comment on the scatter plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bar chart\n",
    "av_mpg.plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box plot\n",
    "data.boxplot(column = 'mpg', by = 'am')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pie chart\n",
    "science = {\n",
    "    'interest': ['Excited', 'Kind of interested', 'OK', 'Not great', 'Bored'],\n",
    "    'before': [19, 25, 40, 5, 11],\n",
    "    'after': [38, 30, 14, 6, 12]\n",
    "}\n",
    "dfscience = pd.DataFrame.from_dict(science).set_index(\"interest\")[['before', 'after']]\n",
    "fig, axs = plt.subplots(1,2, figsize = (8.5,4))\n",
    "dfscience.before.plot(kind=\"pie\", ax=axs[0], labels=None);\n",
    "axs[0].legend(loc=\"upper left\", ncol=5, labels=dfscience.index)\n",
    "dfscience.after.plot(kind=\"pie\", ax=axs[1], labels=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
